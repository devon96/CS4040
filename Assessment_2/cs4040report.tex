\documentclass{csfourzero}

\title{CS4040: Impact of Meltdown \& Spectre mitigations}
\author{Konrad Dryja}
\date{\today}
% A useful package to support on-line references
\usepackage{url}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\lstset{language=C,keywordstyle={\bfseries \color{blue}}}
\bibliographystyle{plain}
\abstract{In my CS4040 report I'd like to evaluate the impact of the discovery of Side-Channel Attacks back in January 2018 with Spectre \& Meltdown on the lead. As those were utilizing many of the performance reliant features - which had to be rolled back or squashed due to security concerns. The removal resulted in many calls about grossly deteriorated performance. With the patches already in place, I have gone ahead and removed them to compare before and after performance of one-core and single-core performance in different tasks, such as prime calculation or rendering. I will be aiming to diversify tested hardware and software, including platforms such as Windows, Linux and Intel, AMD.}


\begin{document}
\maketitle


\section{Introduction}
\label{sec:intro}
With speculative execution no longer considered safe, modern CPU manufacturers were forced to drop these features in favour of enhancing security. But at the same time, it resulted in sacrificing the performance of the chips. Probably the most harming aspect was that speculative execution was at the time an industry standard - after the processor designers hit a ceiling with potential clock speed, being hit by Moore's Law \cite{schaller1997moore}. I will not be focusing on the origin and details of the vulnerabilities, but rather the patches that followed - although will overview the basics in the next section.  

Personally, I found the topic the most interesting, as the aftermath is still haunting security researches to this day, since the fault was not the software, but inherent architecture was found to be flawed. Moreover, frequently we are hearing news how the newest vulnerability based on side-channel execution has been discovered - with the definite fix being complete hardware replacement with a chip produced after 2017. This forced Intel, AMD, ARM to release very aggressive patches, greatly hurting the benchmarks.

This has also raised ethical questions - since some of the affected machines suffered as much as 50\% drop in performance. In the eyes of law, this could classify as false advertising which naturally was followed by many class-action lawsuits. As stated by Intel in their 2017 Annual report, as of February 2018, they were facing 30 customer faced suits along with two securities \cite{intelreport}. Intel perhaps is the company that was the most under fire, since Variant 1, also known as Meltdown, was mostly apparent in their chips, although Variant 2 \& 2 were replicated in almost all consumer chips.

Computing community was faced with a difficult dilemma - how much of security are we willing to sacrifice in favour of increased performance?

\section{Background and related work}
\label{sec:lit}

When speaking about Spectre and Meltdown, it's very important to start from the very beginning - when in July 2017, a researcher Jann Horn from Google's Project Zero has discovered the vulnerability. Due to the severity and potential implications resulting from premature releasing of the findings, those were first communicated directly - on NDA basis - with manufacturers, hoping for an immediate fix. On January 2018, two papers were released by J. Horn et al. illustrating in-depth the vulnerabilities and how they could be replicated \cite{Lipp2018meltdown, Kocher2018spectre}. The papers present a throughout overview of the potential attack - the exploitation here is based on \textbf{Branch Prediction} (BP) along with \textbf{Out of Order Execution} (OOE). Those are optimizations techniques used by almost all CPUs on the market:

\begin{itemize}[noitemsep]
  \item BP lets the processor "predict" direction where the program's execution flow will go towards without explicitly evaluating the condition. For example, in a situation where \lstinline{if} statement was successful for 100 iterations, it can assume that 101st will be successful as well and thus prematurely execute the included code-block, storing the result in L-cache (high speed, low capacity memory located directly on the CPU)  
  \item OOE, on the other hand will often reorder scheduled operations leaving the most time-consuming actions till the end, executing the ones containing required data present in CPU cache immediately - assuming that those do not depend on each other, e.g., it's a simple summation, not utilizing prior information.
\end{itemize}

Together they create a cheap and clever way to speed up the execution of binaries, but unbeknownst opened a pathway for side-channel attacks, exploiting the fact that CPU was executing the code that it wasn't meant to in the first place. Normally, the results are only stored in fast-access L-cache (which is only accessible from kernel-mode and is purely used to increase performance). A proof-of-concept presented by researches illustrated how it's possible to measure how long it takes to access particular variable in order to determine whether it's coming from RAM or L-cache and then make statistical assumption whether the piece of data was evaluated inside of block of code which was executed speculatively. 

There is also a subtle difference between the nature of attack described by \textbf{Meltdown} (Variant 3) and \textbf{Spectre} (Variant 1, 2). The former allowed for any arbirtary process to access kernel-space memory, which contains restricted data such as password or even latest keystroke information. The latter - on the other hand - was capable of crossing process or sandbox boundaries, effectively nullyfying the protections provided by virtualised environment or JIT (just-in-time) compilers. This potentially might allow an attacker to deploy malicious binary onto VM cluster (run by a cloud provider such as GCP or AWD) and access information of other users.

\pagebreak
\begin{lstlisting}[caption=Meltdown PoC,frame=tlrb, numbers=left, firstnumber=1]{Name}
char testArray[256 * 64];
evictCache(testArray); // Clear L-cache
char x = * kernelSpace; // Will cause segmentation fault
testArray[x * 64]++; // Will be executed speculatively
for(int i=0; i<256; i++) {
  // Measure how long it takes to access the element
  if(is_cached(testArray[i * 64]) {
    // Cache hit! Found secret bit.
  }
}
\end{lstlisting}

Multiple patches have been created ever since to mitigate the negative effect, trying to minimize the impact on performance. M. L{\"o}w \cite{low2018overview} provides an overview of created patches and affected hardware. The major and immediate patches which had the biggest performance effect included:
\paragraph{KAISER} \cite{corbet2017current} - which stands for Kernel Page-Table Isolation. Meltdown is exploiting the fact that for the purposes of reduced access times, the entire kernel address page is mapped to every process. This is not directly accessible from user-mode and is protected by privileged bit. Unfortunately, with side-channel attacks, this data can be speculatively loaded and then later retrieved via a covert channel. KAISER aims to drop that mapping and leave only necessary signals.
\paragraph{Retpoline} \cite{turner2018retpoline} - retroactive trampoline, it's a mitigation aiming to catch Branch Predictor in an infinite loop, effectively never speculatively executing sensitive code.
It is also important to mention that software mitigation can only be helpful to a specific point. From month to month more ways of performing side-channel attacks are discovered, bypassing completely the mitigations, thus manufacturers are force to deploy more and more restrictive patches, decreasing the benefits of BP and OOE.


\section{Research question}
\label{sec:rq}
Having introduced the basics of side-channel attacks in previous sections, we're now ready to tackle the implications of the released patches. The research question that would be formed following the aformentioned considerations would be \textbf{"How significant the performance degradation is after mitigations for Side-Channel Attacks were deployed and are they worth the gained security"}

Given the problem context (Section~\ref{sec:intro}) and background
(Section~\ref{sec:lit}), you should now be in a position to present
what you have investigated. \textbf{Pose this as a question.}

Then you should present your approach to addressing this
question.

Guide length: 500 words.

\section{Experimental Design}
\label{sec:exp}

What are your hypotheses? How are you going to test them? What is your
target population? What are your datasets; i.e.\ your sample of the
target population. What are the dependent and independent variables?

Guide length: 500 words.

% \section{Results}
% \label{sec:results}

% Present the results. A good way to organise this is via subsections
% for each hypothesis you tested. Include graphs of results
% (e.g.\ Figure~\ref{fig:data}), tests of significance, etc. If you have
% negative results, include them. A negative results is just as
% informative and useful as a positive one, sometimes more so.

% % \begin{figure}
% % \centerline{\includegraphics[width=5in]{basic-data-errors}}
% % \caption{Some results.}\label{fig:data}
% % \end{figure}

% Guide length: 500 words.

\section{Discussion}
\label{sec:discuss}

What do the results say? What have you learned from the
experiments? Have you identified a correlation between variables, or
causation? What are the limitations of what you've done? What further
experiments might be of benefit?

Guide length: 400 words.

\section{Conclusion}
\label{sec:conc}

What have you done and why? What have you shown through your
experiments?

Guide length: 100 words.

\bibliography{myrefs}

\end{document}
